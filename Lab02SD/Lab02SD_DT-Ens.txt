Lab03: Decision trees (DTs) and Ensemle Learning
=================================================

In this laboratory session, we focus on the principles and implementation of decision tree algorithms and ensemble learning methods.
Two types of decision trees, ID3 and CART, are implemented and analyzed in depth.
Various hyperparameters of decision trees are systematically explored to assess their influence on model performance and generalization.
Furthermore, the experiment extends to ensemble learning techniques, specifically bagging and boosting

TOOLS:
------
Python, Jupyter, pandas, scikit-learn, numpy, timeit, graphviz

DATASETS:
---------
Cars Data

PLAN:
-----
I. Algorithms implementation
    I.1. Probability of a category
    I.2. Homogeneity of a set
    I.3. Set splitting
    I.4. Choice of split feature
    I.5. Splitting feature selection
    I.6. Stopping criterion
    I.7. Final product
II. Application and Analsis
    II.1. Decision trees and Random forests
    II.2. Ensemle Learning
  
WHAT TO DO:
-----------
I. Algorithms implementation
    - Probability of a category                  [4pts]
    - Shannon's entropy                          [4pts]
    - Gini impurity                              [4pts]
    - Information gain                           [4pts]
    - Gini impurity of the split                 [4pts]
    - ID3 splitting feature selection            [4pts]
    - CART splitting feature and value selection [4pts]
    - Stopping criterion                         [4pts]

II. Application and Analsis
    - Feature selection criteria                 [6pts = 2 + 2 + 2]
    - Maximum depth                              [8pts = 2 + 2 + 2 + 2]
    - Minimum leaf samples                       [4pts = 2 + 2]
    - number of estimators (Ensemble)            [8pts = 2 + 2 + 2 + 2]
    - Bootstrap size                             [6pts = 2 + 2 + 2]
    
Progress in class                                [8pts]
Respecting conventions                           [2pts]
Submit the lab on time                           [6pts]
